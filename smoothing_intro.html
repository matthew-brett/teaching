<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>An introduction to smoothing &#8212; Tutorials on imaging, computing and mathematics</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=c058f7c8" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=f2a433a1"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/copybutton.js?v=fc45e087"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Smoothing as convolution" href="smoothing_as_convolution.html" />
    <link rel="prev" title="Slice timing correction" href="slice_timing.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p><span class="math notranslate nohighlight">\(\newcommand{L}[1]{\| #1 \|}\newcommand{VL}[1]{\L{ \vec{#1} }}\newcommand{R}[1]{\operatorname{Re}\,(#1)}\newcommand{I}[1]{\operatorname{Im}\, (#1)}\)</span></p>
<section id="an-introduction-to-smoothing">
<h1>An introduction to smoothing<a class="headerlink" href="#an-introduction-to-smoothing" title="Link to this heading">¶</a></h1>
<p>Smoothing is a process by which data points are averaged with their neighbors
in a series, such as a time series, or image. This (usually) has the effect of
blurring the sharp edges in the smoothed data.  Smoothing is sometimes
referred to as filtering, because smoothing has the effect of suppressing high
frequency signal and enhancing low frequency signal. There are many different
methods of smoothing, but here we discuss smoothing with a Gaussian kernel. We
hope we will succeed in explaining this phrase in the explanation below.</p>
<section id="some-example-data-for-smoothing">
<h2>Some example data for smoothing<a class="headerlink" href="#some-example-data-for-smoothing" title="Link to this heading">¶</a></h2>
<p>First we load and configure the libraries we need:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Make numpy print 4 significant digits for prettiness</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># To get predictable random numbers</span>
</pre></div>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If running in the IPython console, consider running <code class="docutils literal notranslate"><span class="pre">%matplotlib</span></code> to enable
interactive plots.  If running in the Jupyter Notebook, use <code class="docutils literal notranslate"><span class="pre">%matplotlib</span>
<span class="pre">inline</span></code>.</p>
</div>
<p>Here is a set of data, made out of random numbers, that we will use as a
pretend time series, or a single line of data from one plane of an
image.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">n_points</span> <span class="o">=</span> <span class="mi">40</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_points</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_points</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
<span class="go">&lt;...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_intro-2.png">png</a>, <a class="reference external" href=".//smoothing_intro-2.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_intro-2.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_intro-2.png" src="_images/smoothing_intro-2.png" />
</figure>
</section>
<section id="the-gaussian-kernel">
<h2>The Gaussian kernel<a class="headerlink" href="#the-gaussian-kernel" title="Link to this heading">¶</a></h2>
<p>The ‘kernel’ for smoothing, defines the shape of the function that is
used to take the average of the neighboring points. A Gaussian kernel
is a kernel with the shape of a Gaussian (normal distribution) curve.
Here is a standard Gaussian, with a mean of 0 and a <span class="math notranslate nohighlight">\(\sigma\)</span> (=population
standard deviation) of 1.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span> <span class="c1"># x from -6 to 6 in steps of 0.1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">[...]</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_intro-3.png">png</a>, <a class="reference external" href=".//smoothing_intro-3.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_intro-3.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_intro-3.png" src="_images/smoothing_intro-3.png" />
</figure>
<p>In the standard statistical way, we have defined the width of the Gaussian
shape in terms of <span class="math notranslate nohighlight">\(\sigma\)</span>. However, when the Gaussian is used for smoothing,
it is common for imagers to describe the width of the Gaussian with another
related measure, the Full Width at Half Maximum (FWHM).</p>
<p>The FWHM is the width of the kernel, at half of the maximum of the
height of the Gaussian. Thus, for the standard Gaussian above, the
maximum height is ~0.4.  The width of the kernel at 0.2 (on the Y axis) is the
FWHM. As x = -1.175 and 1.175 when y = 0.2, the FWHM is roughly 2.35.</p>
<p>The FWHM is related to sigma by the following formulae (in Python):</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">sigma2fwhm</span><span class="p">(</span><span class="n">sigma</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">sigma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">fwhm2sigma</span><span class="p">(</span><span class="n">fwhm</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">fwhm</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<p>In our case:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sigma2fwhm</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="go">2.3548200450309493</span>
</pre></div>
</div>
</section>
<section id="smoothing-with-the-kernel">
<h2>Smoothing with the kernel<a class="headerlink" href="#smoothing-with-the-kernel" title="Link to this heading">¶</a></h2>
<p>The basic process of smoothing is very simple. We proceed through the
data point by point. For each data point we generate a new value that is
some function of the original value at that point and the surrounding
data points.With Gaussian smoothing, the function that is used is our
Gaussian curve..</p>
<p>So, let us say that we are generating the new, smoothed value for the
14th value in our example data set. We are using a Gaussian with FWHM of
4 units on the x axis. To generate the Gaussian kernel average for this
14th data point, we first move the Gaussian shape to have its center at
13 on the x axis (13 is the 14th value because the first value is 0). In order
to make sure that we don’t do an overall scaling of the values after
smoothing, we divide the values in the Gaussian curve by the total area under
the curve, so that the values add up to one:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">FWHM</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigma</span> <span class="o">=</span> <span class="n">fwhm2sigma</span><span class="p">(</span><span class="n">FWHM</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_position</span> <span class="o">=</span> <span class="mi">13</span> <span class="c1"># 14th point</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_at_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_vals</span> <span class="o">-</span> <span class="n">x_position</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_at_pos</span> <span class="o">=</span> <span class="n">kernel_at_pos</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">kernel_at_pos</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">kernel_at_pos</span><span class="p">)</span>
<span class="go">&lt;...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_intro-6.png">png</a>, <a class="reference external" href=".//smoothing_intro-6.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_intro-6.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_intro-6.png" src="_images/smoothing_intro-6.png" />
</figure>
<p>In fact the Gaussian values for the 12th through 16th data points are:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_at_pos</span><span class="p">[</span><span class="mi">11</span><span class="p">:</span><span class="mi">16</span><span class="p">]</span>
<span class="go">array([ 0.1174,  0.1975,  0.2349,  0.1975,  0.1174])</span>
</pre></div>
</div>
<p>and the data values for the same points are:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y_vals</span><span class="p">[</span><span class="mi">11</span><span class="p">:</span><span class="mi">16</span><span class="p">]</span>
<span class="go">array([-0.2049, -0.3588,  0.6035, -1.6648, -0.7002])</span>
</pre></div>
</div>
<p>We then multiply the Gaussian kernel (weight) values by the values of our
data, and sum the results to get the new smoothed value for point 13:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y_by_weight</span> <span class="o">=</span> <span class="n">y_vals</span> <span class="o">*</span> <span class="n">kernel_at_pos</span> <span class="c1"># element-wise multiplication</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_val</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_by_weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_val</span>
<span class="go">-0.34796859011845732</span>
</pre></div>
</div>
<p>We store this new smoothed value for future use, and move on, to x = 14,
and repeat the process, with the Gaussian kernel now centered over 14.  If we
do this for each point, we eventually get the smoothed version of our original
data. Here is a very inefficient but simple way of doing this:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">smoothed_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y_vals</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">x_position</span> <span class="ow">in</span> <span class="n">x_vals</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_vals</span> <span class="o">-</span> <span class="n">x_position</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">smoothed_vals</span><span class="p">[</span><span class="n">x_position</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_vals</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">smoothed_vals</span><span class="p">)</span>
<span class="go">&lt;...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_intro-10.png">png</a>, <a class="reference external" href=".//smoothing_intro-10.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_intro-10.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_intro-10.png" src="_images/smoothing_intro-10.png" />
</figure>
</section>
<section id="other-kernels">
<h2>Other kernels<a class="headerlink" href="#other-kernels" title="Link to this heading">¶</a></h2>
<p>Of course, we could have used any shape for the kernel - such as a
square wave. A square wave kernel with sum set to one would would have the
effect of replacing each data point with the mean of itself and the
neighboring points.</p>
</section>
<section id="smoothing-as-convolution">
<h2>Smoothing as convolution<a class="headerlink" href="#smoothing-as-convolution" title="Link to this heading">¶</a></h2>
<p>Smoothing can also be implemented and understood as <em>convolution</em> - see
<a class="reference internal" href="smoothing_as_convolution.html"><span class="doc">Smoothing as convolution</span></a> for an explanation.</p>
</section>
<section id="smoothing-in-2d">
<h2>Smoothing in 2D<a class="headerlink" href="#smoothing-in-2d" title="Link to this heading">¶</a></h2>
<p>Smoothing in two dimensions follows simply from smoothing in one
dimension. This time the Gaussian kernel is not a curve, but a cone:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dx</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dy</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">dx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2d</span><span class="p">,</span> <span class="n">y2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x2d</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">y2d</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_2d</span> <span class="o">=</span> <span class="n">kernel_2d</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># unit integral</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x2d</span><span class="p">,</span> <span class="n">y2d</span><span class="p">,</span> <span class="n">kernel_2d</span><span class="p">)</span>
<span class="go">&lt;...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_intro-11.png">png</a>, <a class="reference external" href=".//smoothing_intro-11.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_intro-11.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_intro-11.png" src="_images/smoothing_intro-11.png" />
</figure>
<p>As for the 1D case, we can center this kernel to any point in a 2D plane, and
get the equivalent kernel values for each point on the plane.  Here is a 2D
Gaussian kernel centered at point (10, 10) on a size (20, 20) plane.  See the
page source for the code to make the figure:</p>
<p>(<a class="reference external" href=".//smoothing_intro-13.png">png</a>, <a class="reference external" href=".//smoothing_intro-13.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_intro-13.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_intro-13.png" src="_images/smoothing_intro-13.png" />
</figure>
<p>We then proceed as before, multiplying the values of the kernel (as shown in
the figure above) by the data in the image, to get the smoothed value for that
point, and doing the same for every point on the image.</p>
<p>The procedure is the same for 3D data, except the kernel is rather more
difficult to visualize, being something like a sphere with edges that fade
out, as the cone fades out at the edges in the 2D case.</p>
<p>In fact, it turns out that we don’t have to generate these 2D and 3D versions
of the kernel for the computations, because we can get the same result from
applying a one dimensional smooth sequentially in the 2 or 3 dimensions. Thus,
for 2 dimensions, we could first smooth in the x direction, and then smooth
the x-smoothed data, in the y direction, This gives the same output as
applying the 2D kernel.</p>
</section>
<section id="why-smooth">
<h2>Why smooth?<a class="headerlink" href="#why-smooth" title="Link to this heading">¶</a></h2>
<p>The primary reason for smoothing is to increase signal to noise.  Smoothing
increases signal to noise by the matched filter theorem. This theorem states
that the filter that will give optimum resolution of signal from noise is a
filter that is matched to the signal. In the case of smoothing, the filter is
the Gaussian kernel. Therefore, if we are expecting signal in our images that
is of Gaussian shape, and of FWHM of say 10mm, then this signal will best be
detected after we have smoothed our images with a 10mm FWHM Gaussian
filter.The next few images show the matched filter theorem in action. First we
can generate a simulated signal in a one dimensional set of data, by creating
a Gaussian with FWHM 8 pixels, centered over the 14th data point:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">FWHM</span> <span class="o">=</span> <span class="mi">8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigma</span> <span class="o">=</span> <span class="n">fwhm2sigma</span><span class="p">(</span><span class="n">FWHM</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_position</span> <span class="o">=</span> <span class="mi">13</span> <span class="c1"># 14th point</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sim_signal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_vals</span> <span class="o">-</span> <span class="n">x_position</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">sim_signal</span><span class="p">)</span>
<span class="go">&lt;...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_intro-15.png">png</a>, <a class="reference external" href=".//smoothing_intro-15.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_intro-15.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_intro-15.png" src="_images/smoothing_intro-15.png" />
</figure>
<p>Next, we add some random noise to this signal:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_points</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sim_data</span> <span class="o">=</span> <span class="n">sim_signal</span> <span class="o">+</span> <span class="n">noise</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">sim_data</span><span class="p">)</span>
<span class="go">&lt;...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_intro-16.png">png</a>, <a class="reference external" href=".//smoothing_intro-16.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_intro-16.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_intro-16.png" src="_images/smoothing_intro-16.png" />
</figure>
<p>We then smooth with a matching 8 pixel FWHM filter:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">smoothed_sim_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y_vals</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">x_position</span> <span class="ow">in</span> <span class="n">x_vals</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_vals</span> <span class="o">-</span> <span class="n">x_position</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">smoothed_sim_data</span><span class="p">[</span><span class="n">x_position</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">sim_data</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">smoothed_sim_data</span><span class="p">)</span>
<span class="go">&lt;...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_intro-17.png">png</a>, <a class="reference external" href=".//smoothing_intro-17.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_intro-17.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_intro-17.png" src="_images/smoothing_intro-17.png" />
</figure>
<p>and recover our signal well from the noisy data.</p>
<p>Thus, we smooth with a filter that is of matched size to the activation we
wish to detect. This is of particular relevance when comparing activation
across subjects. Here, the anatomical variability between subjects will mean
that the signal across subjects may be expected to be rather widely
distributed over the cortical surface. In such a case it may be wiser to use a
wide smoothing to detect this signal. In contrast, for a single subject
experiment, where you want to detect (for example) a thalamic signal, which
may be in the order of a few mm across, it would be wiser to use a very narrow
smoothing, or even no smoothing.</p>
</section>
<section id="finding-the-signal-for-any-smoothing-level">
<h2>Finding the signal for any smoothing level<a class="headerlink" href="#finding-the-signal-for-any-smoothing-level" title="Link to this heading">¶</a></h2>
<p>Sometimes you do not know the size or the shape of the signal change
that you are expecting. In these cases, it is difficult to choose a
smoothing level, because the smoothing may reduce signal that is not of
the same size and shape as the smoothing kernel. There are ways of
detecting signal at different smoothing level, that allow appropriate
corrections for multiple corrections, and levels of smoothing. This
Worsley 1996 paper describes such an approach: <a class="reference external" href="http://www.math.mcgill.ca/~keith/scale/scale.abstract.html">Worsley KJ, Marret S,
Neelin P, Evans AC (1996) Searching scale space for activation in PET
images. Human Brain Mapping
4:74-90</a></p>
<p>Matthew Brett (FB) 19/8/99, updated 26 October 2014</p>
<ul class="simple">
<li><p><a class="reference download internal" href="smoothing_intro.py">Download this page as a Python code file</a>;</p></li>
<li><p><a class="reference download internal" href="smoothing_intro.ipynb">Download this page as a Jupyter notebook (no outputs)</a>.</p></li>
</ul>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Teaching</a></h1>



<p class="blurb">Teaching</p>







<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="angle_sum.html">The angle sum rule</a></li>
<li class="toctree-l1"><a class="reference internal" href="bonferroni_correction.html">Notes on the Bonferroni threshold</a></li>
<li class="toctree-l1"><a class="reference internal" href="correlated_regressors.html">Correlated regressors</a></li>
<li class="toctree-l1"><a class="reference internal" href="fdr.html">Thresholding with false discovery rate</a></li>
<li class="toctree-l1"><a class="reference internal" href="floating_point.html">Points on floats</a></li>
<li class="toctree-l1"><a class="reference internal" href="floating_error.html">Floating point error</a></li>
<li class="toctree-l1"><a class="reference internal" href="fourier_basis.html">The Fourier basis</a></li>
<li class="toctree-l1"><a class="reference internal" href="fourier_no_ei.html">Fourier without the ei</a></li>
<li class="toctree-l1"><a class="reference internal" href="fourier_no_ei_orig.html">Fourier without the ei</a></li>
<li class="toctree-l1"><a class="reference internal" href="glm_intro.html">Introduction to the general linear model</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html">The argument in “Why most published research findings are false”</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#the-practice-of-science-is-profoundly-broken-discuss-no-model-and-test">“The practice of science is profoundly broken”. Discuss? - no - model and test!</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#different-ways-of-phrasing-the-argument">Different ways of phrasing the argument</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#some-terms">Some terms</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#what-does-a-significant-statistical-test-result-tell-us">What does a “significant” statistical test result tell us?</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#what-is-a-finding-that-is-likely-to-be-true">What is a finding that is likely to be true?</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#whether-a-finding-is-likely-to-be-true-depends-on-the-power-of-the-experiment">Whether a finding is likely to be true depends on the power of the experiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#quantifying-the-effect-of-bias">Quantifying the effect of bias</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#the-effect-of-multiple-studies">The effect of multiple studies</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#putting-it-together">Putting it together</a></li>
<li class="toctree-l1"><a class="reference internal" href="mutual_information.html">Mutual information as an image matching metric</a></li>
<li class="toctree-l1"><a class="reference internal" href="notation.html">Notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizing_space.html">Calculating transformations between images</a></li>
<li class="toctree-l1"><a class="reference internal" href="on_convolution.html">Convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="on_vectors.html">Vectors and dot products</a></li>
<li class="toctree-l1"><a class="reference internal" href="pca_introduction.html">Introducing principal component analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="simple_complex.html">Refresher on complex numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="slice_timing.html">Slice timing correction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">An introduction to smoothing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#some-example-data-for-smoothing">Some example data for smoothing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-gaussian-kernel">The Gaussian kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#smoothing-with-the-kernel">Smoothing with the kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#other-kernels">Other kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="#smoothing-as-convolution">Smoothing as convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="#smoothing-in-2d">Smoothing in 2D</a></li>
<li class="toctree-l2"><a class="reference internal" href="#why-smooth">Why smooth?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#finding-the-signal-for-any-smoothing-level">Finding the signal for any smoothing level</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="smoothing_as_convolution.html">Smoothing as convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="some_sums.html">Some algebra with summation</a></li>
<li class="toctree-l1"><a class="reference internal" href="sums_of_cosines.html">Sum of sines and cosines</a></li>
<li class="toctree-l1"><a class="reference internal" href="sums_of_sinusoids.html">Sums of sinusoids</a></li>
<li class="toctree-l1"><a class="reference internal" href="random_fields.html">Thresholding with random field theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html">Teaching repo</a></li>
<li class="toctree-l1"><a class="reference internal" href="rotation_2d.html">Formula for rotating a vector in 2D</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector_projection.html">Vector projection</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector_angles.html">Angles between vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="correlation_projection.html">Correlation and projection</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix_rank.html">Matrix rank</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_interpolation.html">Linear interpolation</a></li>
<li class="toctree-l1"><a class="reference internal" href="on_cdfs.html">p values from cumulative distribution functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="functions_are_objects.html">Functions are objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="global_scope.html">Global and local scope of Python variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="brisk_python.html">Brisk introduction to Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="string_formatting.html">Inserting values into strings</a></li>
<li class="toctree-l1"><a class="reference internal" href="on_loops.html">“for” and “while”, “break” and “else:”</a></li>
</ul>


<hr />
<ul>
    
    <li class="toctree-l1"><a href="http://matthew.dynevor.org">Home page</a></li>
    
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="slice_timing.html" title="previous chapter">Slice timing correction</a></li>
      <li>Next: <a href="smoothing_as_convolution.html" title="next chapter">Smoothing as convolution</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2016, Matthew Brett.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.0.2</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/smoothing_intro.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>