<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Smoothing as convolution &#8212; Tutorials on imaging, computing and mathematics</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=c058f7c8" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=f2a433a1"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/copybutton.js?v=fc45e087"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Some algebra with summation" href="some_sums.html" />
    <link rel="prev" title="An introduction to smoothing" href="smoothing_intro.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p><span class="math notranslate nohighlight">\(\newcommand{L}[1]{\| #1 \|}\newcommand{VL}[1]{\L{ \vec{#1} }}\newcommand{R}[1]{\operatorname{Re}\,(#1)}\newcommand{I}[1]{\operatorname{Im}\, (#1)}\)</span></p>
<section id="smoothing-as-convolution">
<h1>Smoothing as convolution<a class="headerlink" href="#smoothing-as-convolution" title="Link to this heading">¶</a></h1>
<p>We load and configure the libraries we need:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Import numerical and plotting libraries</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy.linalg</span> <span class="k">as</span> <span class="nn">npl</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If running in the IPython console, consider running <code class="docutils literal notranslate"><span class="pre">%matplotlib</span></code> to enable
interactive plots.  If running in the Jupyter Notebook, use <code class="docutils literal notranslate"><span class="pre">%matplotlib</span>
<span class="pre">inline</span></code>.</p>
</div>
<section id="smoothing-as-weighted-average">
<h2>Smoothing as weighted average<a class="headerlink" href="#smoothing-as-weighted-average" title="Link to this heading">¶</a></h2>
<p>In the introduction to smoothing tutorial, we had the following random
data:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># To get predictable random numbers</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_points</span> <span class="o">=</span> <span class="mi">40</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_points</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_points</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
<span class="go">&lt;...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_as_convolution-2.png">png</a>, <a class="reference external" href=".//smoothing_as_convolution-2.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_as_convolution-2.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_as_convolution-2.png" src="_images/smoothing_as_convolution-2.png" />
</figure>
<p>In the example, we generated a Gaussian kernel over the x axis at index
13. The kernel had a full-width-at-half-maximum value of 4. This
corresponds to a Gaussian sigma value of about 1.7:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">fwhm2sigma</span><span class="p">(</span><span class="n">fwhm</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">fwhm</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sigma</span> <span class="o">=</span> <span class="n">fwhm2sigma</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigma</span>
<span class="go">1.6986436005760381</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x_position</span> <span class="o">=</span> <span class="mi">13</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Make Gaussian centered at 13 with given sigma</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_at_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_vals</span> <span class="o">-</span> <span class="n">x_position</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Make kernel sum to 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_at_pos</span> <span class="o">=</span> <span class="n">kernel_at_pos</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">kernel_at_pos</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">kernel_at_pos</span><span class="p">)</span>
<span class="go">&lt;...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_as_convolution-5.png">png</a>, <a class="reference external" href=".//smoothing_as_convolution-5.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_as_convolution-5.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_as_convolution-5.png" src="_images/smoothing_as_convolution-5.png" />
</figure>
<p>The new smoothed value for x=13 is the sum of the data y values
(<span class="math notranslate nohighlight">\(y_i : i \in 0, 1, .. 39\)</span>) multiplied by their respective kernel
y values (<span class="math notranslate nohighlight">\(k_i : i \in 0, 1, .. 39\)</span>):</p>
<div class="math notranslate nohighlight">
\[y_{13} = \sum _{i=0} ^{i=39} y_i k_i\]</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_vals</span> <span class="o">*</span> <span class="n">kernel_at_pos</span><span class="p">))</span>
<span class="go">-0.347968590118</span>
</pre></div>
</div>
<p>Of course this is also the <a class="reference external" href="https://en.wikipedia.org/wiki/Dot_product">dot
product</a> of the two
vectors:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">y_vals</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">kernel_at_pos</span><span class="p">))</span>
<span class="go">-0.347968590118</span>
</pre></div>
</div>
</section>
<section id="using-a-finite-width-for-the-kernel">
<h2>Using a finite width for the kernel<a class="headerlink" href="#using-a-finite-width-for-the-kernel" title="Link to this heading">¶</a></h2>
<p>Looking at the plot of the kernel, it looks like we have many zero
values, far from the central x=13 point. Maybe we could be more
efficient, by only doing the y value multiplication for kernel values
that are larger than some threshold, like 0.0001.</p>
<p>Let’s have another look at the Gaussian</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Make a +/- x range large enough to let kernel drop to zero</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_for_kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Calculate kernel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_for_kernel</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Threshold</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_above_thresh</span> <span class="o">=</span> <span class="n">kernel</span> <span class="o">&gt;</span> <span class="mf">0.0001</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Find x values where kernel is above threshold</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_within_thresh</span> <span class="o">=</span> <span class="n">x_for_kernel</span><span class="p">[</span><span class="n">kernel_above_thresh</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_for_kernel</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
<span class="go">[...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x_within_thresh</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="go">[...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">x_within_thresh</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="go">[...]</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_as_convolution-8.png">png</a>, <a class="reference external" href=".//smoothing_as_convolution-8.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_as_convolution-8.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_as_convolution-8.png" src="_images/smoothing_as_convolution-8.png" />
</figure>
<p>We can make a new kernel, with finite width, where the near-zero values
have been trimmed:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">finite_kernel</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">[</span><span class="n">kernel_above_thresh</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Make kernel sum to 1 again</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">finite_kernel</span> <span class="o">=</span> <span class="n">finite_kernel</span> <span class="o">/</span> <span class="n">finite_kernel</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_within_thresh</span><span class="p">,</span> <span class="n">finite_kernel</span><span class="p">)</span>
<span class="go">[...]</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_as_convolution-9.png">png</a>, <a class="reference external" href=".//smoothing_as_convolution-9.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_as_convolution-9.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_as_convolution-9.png" src="_images/smoothing_as_convolution-9.png" />
</figure>
<p>This kernel has a finite width:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">finite_kernel</span><span class="p">)</span>
<span class="go">15</span>
</pre></div>
</div>
<p>To get our smoothed value for x=13, we can shift this trimmed kernel be
centered over x=13, and only multiply by the y values that are within
the kernel width:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Number of kernel points before center (at 0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_n_below_0</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">finite_kernel</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_n_below_0</span>
<span class="go">7</span>
</pre></div>
</div>
<p>Because we cut the kernel at a low threshold, the result from using the
finite kernel is very similar to using the infinite kernel that we used
above:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Multiply and sum y values within the finite kernel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kernel_starts_at</span> <span class="o">=</span> <span class="mi">13</span> <span class="o">-</span> <span class="n">kernel_n_below_0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_within_kernel</span> <span class="o">=</span> <span class="n">y_vals</span><span class="p">[</span><span class="n">kernel_starts_at</span> <span class="p">:</span> <span class="n">kernel_starts_at</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">finite_kernel</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">finite_kernel</span><span class="p">,</span> <span class="n">y_within_kernel</span><span class="p">))</span>
<span class="go">-0.347973672994</span>
</pre></div>
</div>
</section>
<section id="id1">
<h2>Smoothing as convolution<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h2>
<p>If are you <a class="reference internal" href="on_convolution.html"><span class="doc">familiar with convolution</span></a> the smoothing
procedure may be familiar.</p>
<p>With convolution, we also have a kernel, and we also generate values by
taking the sum of the products of values within the kernel.</p>
<p>With convolution, we <em>reverse</em> the convolution <em>kernel</em> and the step
through the y values, cross-multiplying the y signal with the reversed
kernel.</p>
<p>That could work here too. There is no need for us to reverse the kernel,
because it is symmetrical.</p>
<p>In fact, it might be possible to see that, we can get exactly our
required result for x=13, by convolving the y values with the finite
smoothing kernel.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">convolved_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">convolve</span><span class="p">(</span><span class="n">y_vals</span><span class="p">,</span> <span class="n">finite_kernel</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">convolved_y</span><span class="p">[</span><span class="mi">13</span><span class="o">+</span> <span class="n">kernel_n_below_0</span><span class="p">])</span>
<span class="go">-0.347973672994</span>
</pre></div>
</div>
<p>Why have I printed out the value at <code class="docutils literal notranslate"><span class="pre">13</span> <span class="pre">+</span> <span class="pre">kernel_n_below_0</span></code> ? Because
this is the convolution value that corresponds to the weighted sum we
did with our original multiplication. When the convolution algorithm
gets to this index, it applies the reversed smoothing kernel to this
index and the <code class="docutils literal notranslate"><span class="pre">len(finite_kernel)</span> <span class="pre">-</span> <span class="pre">1</span></code> values before it. This is the
exact same set of multiplications we did for the original
multiplication. Thus, in order to get the same smoothed values as we did
when we were multiplying by a centered kernel, we have to get the values
from the convolved output from half the kernel width ahead of the index
we are interested in.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">smoothed_by_convolving</span> <span class="o">=</span> <span class="n">convolved_y</span><span class="p">[</span><span class="n">kernel_n_below_0</span><span class="p">:(</span><span class="n">n_points</span><span class="o">+</span><span class="n">kernel_n_below_0</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">smoothed_by_convolving</span><span class="p">)</span>
<span class="go">&lt;...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_as_convolution-14.png">png</a>, <a class="reference external" href=".//smoothing_as_convolution-14.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_as_convolution-14.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_as_convolution-14.png" src="_images/smoothing_as_convolution-14.png" />
</figure>
<p>Here we were able to get the effect of an offset in the kernel, by
taking an offset (<code class="docutils literal notranslate"><span class="pre">kernel_n_below_0</span></code>) in the output data. We have made
use of the <a class="reference external" href="https://en.wikipedia.org/wiki/Convolution#Translation_invariance">translation
invariance</a>
property of convolution.</p>
</section>
<section id="convolution-and-edges">
<h2>Convolution and edges<a class="headerlink" href="#convolution-and-edges" title="Link to this heading">¶</a></h2>
<p>If you were very observant, you may have noticed that the convolution
results above differ slightly from the convolution using the simple
crude method in the <a class="reference internal" href="smoothing_intro.html"><span class="doc">An introduction to smoothing</span></a>.</p>
<p>Here are those results for comparison:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">smoothed_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y_vals</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">x_position</span> <span class="ow">in</span> <span class="n">x_vals</span><span class="p">:</span>
<span class="gp">... </span>     <span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_vals</span> <span class="o">-</span> <span class="n">x_position</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">... </span>     <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
<span class="gp">... </span>     <span class="n">smoothed_vals</span><span class="p">[</span><span class="n">x_position</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_vals</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">smoothed_vals</span><span class="p">)</span>
<span class="go">&lt;...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href=".//smoothing_as_convolution-15.png">png</a>, <a class="reference external" href=".//smoothing_as_convolution-15.hires.png">hires.png</a>, <a class="reference external" href=".//smoothing_as_convolution-15.pdf">pdf</a>)</p>
<figure class="align-default">
<img alt="_images/smoothing_as_convolution-15.png" src="_images/smoothing_as_convolution-15.png" />
</figure>
<p>Notice that this plot has higher values at the edges of the data.</p>
<p>The reason is that the simple method above only evaluates the kernel for
the x points present in the data. Therefore, at the left and right
edges, this method is only applying half a Gaussian to the data. On the
left it is applying the right half of the Gaussian, and on the right it
is applying the left half of the Gaussian. Notice too that this simple
method always makes the kernel sum to zero, so, when smoothing the
points at the edges, with the half kernel, the remaining points get more
weight.</p>
<p>This is one technique for dealing with the edges called <em>truncating the
kernel</em>.</p>
<p>Convolution, by default, does not truncate the kernel, but assumes that
data outside the x points we have are all zero. This is called <em>zero
padding</em>. Using zero padding, the points towards the edge get pulled
down towards zero because they are part-made of the result of taking the
product of zero with the kernel values.</p>
<p>When we do spatial smoothing, this can be a significant problem. For
example, imagine smoothing close to the bottom (inferior) edge of a
brain image, where the edge voxels are likely to have brain signal. If
we use zero padding then the values near the edge will get pulled
towards zero causing a strong signal change from smoothing.</p>
<p>In this case we might prefer some other method of dealing with the data
off the edge of the image, for example by assuming the signal is a
flipped version of the signal going towards the edge. See the
description of the <code class="docutils literal notranslate"><span class="pre">mode</span></code> argument in the docstring for
<code class="docutils literal notranslate"><span class="pre">scipy.ndimage.gaussian_filter</span></code> for some other options.</p>
<ul class="simple">
<li><p><a class="reference download internal" href="smoothing_as_convolution.py">Download this page as a Python code file</a>;</p></li>
<li><p><a class="reference download internal" href="smoothing_as_convolution.ipynb">Download this page as a Jupyter notebook (no outputs)</a>.</p></li>
</ul>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Teaching</a></h1>



<p class="blurb">Teaching</p>







<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="angle_sum.html">The angle sum rule</a></li>
<li class="toctree-l1"><a class="reference internal" href="bonferroni_correction.html">Notes on the Bonferroni threshold</a></li>
<li class="toctree-l1"><a class="reference internal" href="correlated_regressors.html">Correlated regressors</a></li>
<li class="toctree-l1"><a class="reference internal" href="fdr.html">Thresholding with false discovery rate</a></li>
<li class="toctree-l1"><a class="reference internal" href="floating_point.html">Points on floats</a></li>
<li class="toctree-l1"><a class="reference internal" href="floating_error.html">Floating point error</a></li>
<li class="toctree-l1"><a class="reference internal" href="fourier_basis.html">The Fourier basis</a></li>
<li class="toctree-l1"><a class="reference internal" href="fourier_no_ei.html">Fourier without the ei</a></li>
<li class="toctree-l1"><a class="reference internal" href="fourier_no_ei_orig.html">Fourier without the ei</a></li>
<li class="toctree-l1"><a class="reference internal" href="glm_intro.html">Introduction to the general linear model</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html">The argument in “Why most published research findings are false”</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#the-practice-of-science-is-profoundly-broken-discuss-no-model-and-test">“The practice of science is profoundly broken”. Discuss? - no - model and test!</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#different-ways-of-phrasing-the-argument">Different ways of phrasing the argument</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#some-terms">Some terms</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#what-does-a-significant-statistical-test-result-tell-us">What does a “significant” statistical test result tell us?</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#what-is-a-finding-that-is-likely-to-be-true">What is a finding that is likely to be true?</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#whether-a-finding-is-likely-to-be-true-depends-on-the-power-of-the-experiment">Whether a finding is likely to be true depends on the power of the experiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#quantifying-the-effect-of-bias">Quantifying the effect of bias</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#the-effect-of-multiple-studies">The effect of multiple studies</a></li>
<li class="toctree-l1"><a class="reference internal" href="ioannidis_2005.html#putting-it-together">Putting it together</a></li>
<li class="toctree-l1"><a class="reference internal" href="mutual_information.html">Mutual information as an image matching metric</a></li>
<li class="toctree-l1"><a class="reference internal" href="notation.html">Notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizing_space.html">Calculating transformations between images</a></li>
<li class="toctree-l1"><a class="reference internal" href="on_convolution.html">Convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="on_vectors.html">Vectors and dot products</a></li>
<li class="toctree-l1"><a class="reference internal" href="pca_introduction.html">Introducing principal component analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="simple_complex.html">Refresher on complex numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="slice_timing.html">Slice timing correction</a></li>
<li class="toctree-l1"><a class="reference internal" href="smoothing_intro.html">An introduction to smoothing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Smoothing as convolution</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#smoothing-as-weighted-average">Smoothing as weighted average</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-a-finite-width-for-the-kernel">Using a finite width for the kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Smoothing as convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="#convolution-and-edges">Convolution and edges</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="some_sums.html">Some algebra with summation</a></li>
<li class="toctree-l1"><a class="reference internal" href="sums_of_cosines.html">Sum of sines and cosines</a></li>
<li class="toctree-l1"><a class="reference internal" href="sums_of_sinusoids.html">Sums of sinusoids</a></li>
<li class="toctree-l1"><a class="reference internal" href="random_fields.html">Thresholding with random field theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html">Teaching repo</a></li>
<li class="toctree-l1"><a class="reference internal" href="rotation_2d.html">Formula for rotating a vector in 2D</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector_projection.html">Vector projection</a></li>
<li class="toctree-l1"><a class="reference internal" href="vector_angles.html">Angles between vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="correlation_projection.html">Correlation and projection</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrix_rank.html">Matrix rank</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_interpolation.html">Linear interpolation</a></li>
<li class="toctree-l1"><a class="reference internal" href="on_cdfs.html">p values from cumulative distribution functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="functions_are_objects.html">Functions are objects</a></li>
<li class="toctree-l1"><a class="reference internal" href="global_scope.html">Global and local scope of Python variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="brisk_python.html">Brisk introduction to Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="string_formatting.html">Inserting values into strings</a></li>
<li class="toctree-l1"><a class="reference internal" href="on_loops.html">“for” and “while”, “break” and “else:”</a></li>
</ul>


<hr />
<ul>
    
    <li class="toctree-l1"><a href="http://matthew.dynevor.org">Home page</a></li>
    
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="smoothing_intro.html" title="previous chapter">An introduction to smoothing</a></li>
      <li>Next: <a href="some_sums.html" title="next chapter">Some algebra with summation</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2016, Matthew Brett.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.0.2</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/smoothing_as_convolution.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>